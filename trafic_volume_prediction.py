# -*- coding: utf-8 -*-
"""Trafic_volume_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G2ZKNaCgS5va2g2IKhnaiJ-ojR5TUzqh
"""

# traffic_predict_signal.py
"""
Simple Traffic Volume Predictor + Rule-based Signal Adjustment
- Loads Metro_Interstate_Traffic_Volume.csv if present (Kaggle dataset).
- Otherwise generates a synthetic hourly dataset.
- Trains XGBoost regressor (time-series split).
- Evaluates model and suggests simple traffic-signal green-time changes.
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import timedelta
from sklearn.metrics import mean_absolute_error, mean_squared_error
from xgboost import XGBRegressor
import joblib

sns.set_style("whitegrid")

# ------------- Step 0: Settings -------------
DATA_PATH = "/content/Metro_Interstate_Traffic_Volume.csv"  # place the Kaggle CSV here if you have it
RANDOM_STATE = 42
TEST_FRACTION = 0.2   # last 20% of time series for testing
LAGS = [1, 2, 3, 24, 168]  # hourly lags (1h, 2h, 3h, 1 day, 1 week)
ROLL_WINDOWS = [3, 24]
MODEL_PATH = "xgb_traffic_model.pkl"
PRED_CSV = "predictions.csv"

# ------------- Step 1: Load or simulate data -------------
def load_or_simulate():
    if os.path.exists(DATA_PATH):
        print(f"Loading dataset from {DATA_PATH}")
        df = pd.read_csv(DATA_PATH)
        # Dataset expected to have 'date_time' and 'traffic_volume' (and optional weather columns)
        if 'date_time' not in df.columns or 'traffic_volume' not in df.columns:
            raise ValueError("CSV must contain 'date_time' and 'traffic_volume' columns.")
        df['date_time'] = pd.to_datetime(df['date_time'])
        df = df.sort_values('date_time').set_index('date_time')
        # keep relevant columns (if present)
        # expected optional weather features: temp, rain_1h, clouds_all
        possible_weather = ['temp', 'rain_1h', 'snow_1h', 'clouds_all']
        for col in possible_weather:
            if col not in df.columns:
                df[col] = np.nan
        return df
    else:
        print("Dataset not found â€” generating synthetic hourly traffic dataset (90 days).")
        rng = pd.date_range(start='2020-01-01', periods=24*90, freq='H')  # 90 days hourly
        base = 1000
        # daily pattern: peak in morning (8-9) and evening (17-19)
        hours = rng.hour
        daily_pattern = 800 * (np.exp(-0.5*((hours - 8)/2)**2) + np.exp(-0.5*((hours - 18)/2.5)**2))
        # weekly pattern: weekends lower
        dow = rng.dayofweek
        weekly_factor = np.where(dow >= 5, 0.7, 1.0)
        # seasonal noise & random events
        noise = np.random.normal(scale=150, size=len(rng))
        # occasional 'event' spikes
        event = np.zeros(len(rng))
        for i in range(5):
            idx = np.random.randint(0, len(rng))
            event[idx:idx+6] += np.random.randint(500, 1500)  # event lasts a few hours
        traffic_volume = np.clip(base * weekly_factor + daily_pattern + noise + event, a_min=50, a_max=None).astype(int)
        # simple weather features (temp, rain)
        temp = 15 + 10*np.sin(2*np.pi*(rng.dayofyear/365)) + np.random.normal(scale=2, size=len(rng))
        rain_1h = np.random.choice([0,0,0,0,1,2,5,10], size=len(rng), p=[0.7,0.1,0.06,0.05,0.03,0.03,0.02,0.01])
        df = pd.DataFrame({'traffic_volume': traffic_volume, 'temp': temp, 'rain_1h': rain_1h}, index=rng)
        df.index.name = 'date_time'
        return df

df = load_or_simulate()
print("Data shape:", df.shape)
print(df.head())

# ------------- Step 2: Feature engineering -------------
def create_features(df):
    df = df.copy()
    # ensure datetime index
    if not isinstance(df.index, pd.DatetimeIndex):
        raise ValueError("Dataframe must have DatetimeIndex on 'date_time'.")
    df['hour'] = df.index.hour
    df['dayofweek'] = df.index.dayofweek
    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
    df['month'] = df.index.month
    # lags
    for lag in LAGS:
        df[f'lag_{lag}'] = df['traffic_volume'].shift(lag)
    # rolling means
    for w in ROLL_WINDOWS:
        df[f'roll_mean_{w}'] = df['traffic_volume'].rolling(window=w, min_periods=1).mean().shift(1)
    # if weather columns exist, keep them, else NaN handled later
    # fill missing (do simple forward fill for rolling features)
    df = df.dropna(subset=['lag_1'])  # ensure lags exist
    # optional: log transform target to stabilize
    df['traffic_volume_log'] = np.log1p(df['traffic_volume'])
    return df

df_fe = create_features(df)
print("After feature engineering:", df_fe.shape)
print(df_fe[[ 'traffic_volume', 'lag_1', 'lag_24', 'roll_mean_24']].head())

# ------------- Step 3: Train-test split (time-based) -------------
n = len(df_fe)
train_n = int((1 - TEST_FRACTION) * n)
train_df = df_fe.iloc[:train_n]
test_df = df_fe.iloc[train_n:]

# features to use (include weather if present)
candidate_features = ['hour', 'dayofweek', 'is_weekend', 'month'] + \
                     [f'lag_{l}' for l in LAGS] + \
                     [f'roll_mean_{w}' for w in ROLL_WINDOWS]
# include weather if present in columns
for wcol in ['temp', 'rain_1h', 'snow_1h', 'clouds_all']:
    if wcol in df_fe.columns:
        candidate_features.append(wcol)

X_train = train_df[candidate_features]
y_train = train_df['traffic_volume']  # or 'traffic_volume_log' if modeling log
X_test = test_df[candidate_features]
y_test = test_df['traffic_volume']

print("Train size:", X_train.shape, "Test size:", X_test.shape)

pip install -U xgboost

import xgboost
print(xgboost.__version__)

# ------------- Step 4: Train XGBoost model -------------
model = XGBRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=RANDOM_STATE,
    objective='reg:squarederror'
)

print("Training XGBoost... (this may take a while depending on data size)")
model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=50
)
print("Training complete.")

# ------------- Step 5: Evaluate -------------
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
# MAPE (avoid division by zero)
eps = 1e-6
mape = np.mean(np.abs((y_test - y_pred) / (y_test + eps))) * 100

print(f"MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")

# combine predictions back to dataframe
results = test_df.copy()
results = results.assign(predicted_volume = y_pred)
results = results[['traffic_volume', 'predicted_volume'] + [c for c in candidate_features if c in results.columns]]

results.to_csv(PRED_CSV)
print(f"Saved predictions to {PRED_CSV}")

# ------------- Step 6: Plots (actual vs predicted for the last week) -------------
def plot_actual_vs_pred(results_df, last_hours=24*7):
    plt.figure(figsize=(14,5))
    subset = results_df.iloc[-last_hours:]
    plt.plot(subset.index, subset['traffic_volume'], label='Actual', linewidth=2, marker='o', markersize=4)
    plt.plot(subset.index, subset['predicted_volume'], label='Predicted', linewidth=2, marker='x', markersize=4)
    plt.title("Actual vs Predicted Traffic Volume (last {} hours)".format(last_hours))
    plt.xlabel("Time")
    plt.ylabel("Traffic Volume")
    plt.legend()
    plt.tight_layout()
    plt.show()

plot_actual_vs_pred(results, last_hours=min(24*7, len(results)))

# ------------- Step 7: Simple congestion threshold + signal suggestion -------------
# choose a threshold (e.g., 85th percentile) or domain-specific number
threshold = int(df['traffic_volume'].quantile(0.85))
print(f"Using congestion threshold = {threshold} vehicles/hour (85th percentile).")

results['congested_actual'] = results['traffic_volume'] > threshold
results['congested_pred'] = results['predicted_volume'] > threshold

# simple function to suggest green time based on predicted volume
def suggest_green_time(predicted_volume, base_green=30, threshold=threshold):
    """
    Return new green time (seconds). Very simple rule-based logic:
     - If predicted volume > threshold: increase green by an amount proportional to excess
     - Else if low traffic: reduce green slightly
    """
    if predicted_volume > threshold:
        # linear scaling for extra time (clamp between +5 and +30 seconds)
        factor = (predicted_volume - threshold) / threshold
        add_seconds = int(np.clip(5 + 25 * factor, 5, 30))
        return base_green + add_seconds
    elif predicted_volume < 0.5 * threshold:
        # reduce green if very low traffic
        return max(10, base_green - 10)
    else:
        return base_green

# apply sample suggestions to last N timesteps in results
sample = results.iloc[-24*3:].copy()  # last 3 days of predictions
sample['suggested_green'] = sample['predicted_volume'].apply(lambda v: suggest_green_time(v, base_green=30, threshold=threshold))
print(sample[['traffic_volume','predicted_volume','congested_pred','suggested_green']].head(10))

# ------------- Step 8: Save model -------------
joblib.dump(model, MODEL_PATH)
print(f"Saved model to {MODEL_PATH}")

# ------------- Step 9: Short summary of results -------------
print("Summary:")
print(f" - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")
print(f" - Predictions saved to: {PRED_CSV}")
print(f" - Model saved to: {MODEL_PATH}")
print("Done.")

# ------------- Step 10: Scatter plot of Actual vs Predicted -------------
plt.figure(figsize=(8, 8))
plt.scatter(results['traffic_volume'], results['predicted_volume'], color='blue', alpha=0.5)
plt.title("Actual vs Predicted Traffic Volume (Test Set)")
plt.xlabel("Actual Traffic Volume")
plt.ylabel("Predicted Traffic Volume")
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Take only first 10 samples
sample_results = results.head(20)

plt.figure(figsize=(8, 6))
plt.scatter(range(len(sample_results)), sample_results['traffic_volume'], color='blue', label='Actual', s=80)
plt.scatter(range(len(sample_results)), sample_results['predicted_volume'], color='red', label='Predicted', s=80)
plt.title("Actual vs Predicted Traffic Volume")
plt.xlabel("Data")
plt.ylabel("Traffic Volume")
plt.legend()
plt.grid(True)
plt.show()

sns.heatmap(df[['traffic_volume','rain_1h','temp']].corr(), annot=True)
plt.show()

import shap
explainer = shap.Explainer(model)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)